= Chapter 2: The Language Barrier
:description: How language becomes numbers that computers can process

_Part I: The Magic Trick_

'''

We've established that LLMs predict the next word. But there's a problem.

Computers don't understand words.

== Calculators, Not Readers

At their core, computers are calculators. They add, subtract, multiply. They compare numbers. They shuffle bits around. That's it.

A computer doesn't see the word "Paris" and think of baguettes and art museums. It sees... well, it can't see "Paris" at all. Letters mean nothing to a processor.

So we face a fundamental challenge:

*How do we turn words — arbitrary symbols that humans invented — into something a calculator can work with?*

== The Translation Pipeline

The answer is a multi-step translation process. Here's the preview:

[source]
----
Your Text
    ↓
Break into pieces (tokens)
    ↓
Assign numbers (token IDs)
    ↓
Look up meanings (embeddings)
    ↓
Do math (transformer layers)
    ↓
Score possible next words
    ↓
Pick one
    ↓
Response Text
----

Every single thing that happens inside an LLM is math. No exceptions.

The genius is in how we convert language into math — and back again — while preserving meaning along the way.

== An Analogy: Translating a Book into Music

Imagine you wanted to turn a novel into a symphony. You might:

. *Break it into syllables* — manageable pieces
. *Assign each syllable a note* — arbitrary but consistent
. *Arrange notes into melodies* — capture the emotional arc
. *Compose the full symphony* — complex but derived from the original

This is roughly what an LLM does with language:

. Break text into tokens (pieces)
. Assign each token an ID (number)
. Convert IDs into embeddings (meaningful vectors)
. Process through transformer layers (the symphony)

The next few chapters explore each step.

'''

*You might be wondering:* "_How does turning a book into music relate to tokenization and embeddings? This seems abstract._"

The analogy maps concrete steps: syllables → tokens, notes → token IDs, melodies → embeddings, symphony → transformer processing. It's a structural parallel showing how we break language into pieces, assign identifiers, create meaningful representations, and then process them. The point isn't that LLMs are literally like music — it's that both involve transforming one representation into another while preserving something essential.

'''

== Pause and Reflect

Before reading on, make a guess:

_What's the simplest way you'd turn a word into a number?_

Maybe assign "a" = 1, "b" = 2, "c" = 3?

Maybe count the letters?

Maybe give each word a unique number?

Keep your guess in mind. We'll see how close you were.

== Chapter Takeaway

[NOTE]
====
*LLMs bridge the gap between human language and machine math.* Every word becomes a number — but not just any number. The conversion is designed to preserve meaning, so that math on the numbers corresponds to reasoning about the concepts.
====

== Part I Summary

You've learned the two foundational truths:

. *LLMs predict the next token.* That's the entire job. Understanding emerges from prediction at scale.
. *Computers only do math.* Language must be converted into numbers before any processing can happen.

Now we dive into the details: How exactly do we turn words into numbers?

xref:03-tokenization.adoc[Next: Breaking Language into Pieces →]
