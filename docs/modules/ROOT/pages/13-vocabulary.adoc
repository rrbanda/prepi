= Chapter 13: The Complete Vocabulary
:description: A comprehensive reference of all LLM terms and concepts covered in this book

_Part VII: Mastery_

'''

You've covered a lot of ground. This chapter consolidates every term into a single reference.

Use this as a quick lookup when reviewing concepts or discussing LLMs with colleagues.

== Core Concepts

[cols="1,2,1", options="header"]
|===
|Term
|Plain English Definition
|Covered In

|*LLM*
|Large Language Model — a model that predicts the next token
|Chapter 1

|*Token*
|A piece of text (word, subword, or character)
|Chapter 3

|*Token ID*
|An integer label for a token (no meaning)
|Chapter 3

|*Vocabulary*
|The fixed set of all possible tokens
|Chapter 3

|*Tokenizer*
|The tool that breaks text into token IDs
|Chapter 3

|*BPE*
|Byte Pair Encoding — common tokenization algorithm
|Chapter 3
|===

== Representations

[cols="1,2,1", options="header"]
|===
|Term
|Plain English Definition
|Covered In

|*Vector*
|A list of numbers; a point in space
|Chapter 4

|*Embedding*
|A meaningful vector for a token
|Chapter 4

|*Embedding Matrix*
|Lookup table: Token ID → Embedding
|Chapter 4

|*Tensor*
|A container for numbers (1D=vector, 2D=matrix, nD=tensor)
|Chapter 4

|*Contextual Representation*
|A vector that includes information from context
|Chapter 8
|===

== Model Structure

[cols="1,2,1", options="header"]
|===
|Term
|Plain English Definition
|Covered In

|*Neural Network*
|A function that transforms inputs through layers
|Chapter 5

|*Layer*
|A step of transformation (input → output)
|Chapter 5

|*Weights*
|Learned numbers that control transformations
|Chapter 5

|*Architecture*
|The blueprint of the model (layers, sizes, connections)
|Chapter 5

|*Layer Normalization*
|Normalizes activations to mean=0, variance=1 for stable training
|Chapter 5

|*Pre-LayerNorm*
|GPT's approach: normalize _before_ attention/FFN, not after
|Chapter 5

|*GELU*
|Gaussian Error Linear Unit — GPT's smooth activation function
|Chapter 5

|*ReLU*
|Rectified Linear Unit — simpler activation (max(0, x))
|Chapter 5

|*Residual Connection*
|Adding input to output (x + layer(x)) to help gradient flow
|Chapter 5

|*Transformer*
|The architecture that uses self-attention
|Chapter 6

|*Transformer Block*
|One round of attention + FFN with residuals and normalization
|Chapter 6

|*Feed-Forward Network (FFN)*
|Two-layer MLP in each block (expands 4×, then contracts)
|Chapter 6

|*Decoder-only*
|Transformer that generates left-to-right
|Chapter 6
|===

== Attention Mechanism

[cols="1,2,1", options="header"]
|===
|Term
|Plain English Definition
|Covered In

|*Self-Attention*
|Mechanism for tokens to "see" each other
|Chapter 7

|*Query (Q)*
|"What am I looking for?"
|Chapter 7

|*Key (K)*
|"What do I offer/contain?"
|Chapter 7

|*Value (V)*
|"Here's my actual information"
|Chapter 7

|*Attention Score*
|How relevant one token is to another
|Chapter 7

|*Attention Weights*
|Probabilities derived from attention scores
|Chapter 7

|*Multi-Head Attention*
|Multiple parallel attention computations
|Chapter 7

|*Causal Masking*
|Preventing tokens from seeing future tokens
|Chapter 7
|===

== Context

[cols="1,2,1", options="header"]
|===
|Term
|Plain English Definition
|Covered In

|*Context Window*
|Maximum number of tokens the model can see
|Chapter 8

|*Contextual Representation*
|A token's vector after attention (context-aware)
|Chapter 8

|*Position Encoding*
|Information added to embeddings about token position
|Chapter 8

|*RoPE*
|Rotary Position Embeddings — a position encoding method
|Chapter 8
|===

== Inference

[cols="1,2,1", options="header"]
|===
|Term
|Plain English Definition
|Covered In

|*Inference*
|Running the model to generate output
|Chapter 9

|*Forward Pass*
|Data flowing through the model from input to output
|Chapter 9

|*Logits*
|Raw scores for each vocabulary token before softmax
|Chapter 9

|*Softmax*
|Function that converts logits to probabilities
|Chapter 9

|*Weight Tying*
|Sharing embedding matrix with output projection (saves params)
|Chapter 9

|*Sampling*
|Choosing the next token based on probabilities
|Chapter 9

|*Autoregressive*
|Generating one token at a time, each based on previous
|Chapter 9

|*Temperature*
|Parameter controlling randomness in sampling
|Chapter 9

|*Top-k Sampling*
|Sample only from the k most likely tokens
|Chapter 9

|*Top-p / Nucleus*
|Sample only from tokens comprising top p% probability
|Chapter 9

|*Beam Search*
|Keep multiple candidate sequences, pick best at end
|Chapter 9

|*Greedy Decoding*
|Always choosing the highest probability token
|Chapter 9
|===

== Training

[cols="1,2,1", options="header"]
|===
|Term
|Plain English Definition
|Covered In

|*Training*
|The process of learning weights from data
|Chapter 10

|*Loss*
|A number measuring how wrong the prediction was
|Chapter 10

|*Cross-Entropy Loss*
|Common loss function: -log(probability of correct token)
|Chapter 10

|*Perplexity*
|exp(loss) — "how many choices the model is confused between"
|Chapter 10

|*Backpropagation*
|Computing how each weight affects the loss
|Chapter 10

|*Gradient*
|Direction to adjust a weight to reduce loss
|Chapter 10

|*Gradient Clipping*
|Capping gradient magnitude to prevent exploding updates
|Chapter 10

|*Learning Rate*
|How big the weight adjustments are
|Chapter 10

|*AdamW*
|Standard optimizer for LLMs (Adam with weight decay)
|Chapter 10

|*Learning Rate Schedule*
|Warmup then decay of learning rate during training
|Chapter 10

|*Training Data*
|The text used to train the model
|Chapter 10
|===

== Optimization

[cols="1,2,1", options="header"]
|===
|Term
|Plain English Definition
|Covered In

|*KV Cache*
|Stored Key/Value vectors to avoid recomputation
|Chapter 11

|*Batching*
|Processing multiple requests together
|Chapter 12

|*Continuous Batching*
|Dynamically adding/removing requests from batch
|Chapter 12

|*PagedAttention*
|vLLM's method for non-contiguous KV cache storage
|Chapter 12

|*Throughput*
|Tokens generated per second (system-wide)
|Chapter 12

|*Latency*
|Time to generate response
|Chapter 12

|*TTFT*
|Time To First Token
|Chapter 12
|===

== Serving Systems

[cols="1,1", options="header"]
|===
|Term
|What It Is

|*vLLM*
|Open-source serving system with PagedAttention

|*TensorRT-LLM*
|NVIDIA's optimized serving library

|*TGI*
|HuggingFace's Text Generation Inference

|*Triton*
|NVIDIA's inference server
|===

== Quick Reference Card

Memorize these key relationships:

[source]
----
Text
  ↓ Tokenizer
Token IDs (integers, no meaning)
  ↓ Embedding Matrix
Embeddings (vectors, meaning)
  ↓ + Position Encodings
Input to Transformer
  ↓ Self-Attention (Q × K → weights → blend V)
  ↓ Feed-Forward Network
  ↓ × N layers
Contextual Vectors
  ↓ Output Projection
Logits (scores for all vocab)
  ↓ Softmax
Probabilities
  ↓ Sampling
Next Token
  ↓ Repeat
Full Response
----

== Chapter Takeaway

[NOTE]
====
This vocabulary reference covers all major terms from the book. Use it for quick lookups during review. Understanding the relationships between terms is as important as knowing definitions.
====

xref:14-why-it-works.adoc[Next: The Deep Insight — Why It Works →]
