= Chapter 13: The OpenShift AI Platform
:description: Understanding the architecture and components of Red Hat OpenShift AI 3.0

_Part VI: Under the Hood_

'''

You've learned how inference works (Chapter 9), how training happens (Chapter 10), and how serving systems optimize production deployment (Chapter 12). But how do all these pieces come together in a unified platform?

*Red Hat OpenShift AI* is an enterprise AI platform that orchestrates the complete AI/ML lifecycle—from development through production serving. This chapter explores its architecture and how it implements the concepts you've learned.

== Platform Architecture

OpenShift AI 3.0 consists of two layers:

=== Management Layer

The *Red Hat OpenShift AI Operator* is a meta-operator that deploys and maintains all platform components. Think of it as the "operator of operators"—it manages the lifecycle of sub-operators, ensures component health, and coordinates updates.

[source]
----
Red Hat OpenShift AI Operator
           │
           ├── Dashboard Operator
           ├── Model Registry Operator
           ├── KServe Controller
           ├── Training Operators (Kueue, Ray, etc.)
           └── Monitoring Components
----

=== Service Layer

The service layer contains the actual AI/ML capabilities, organized by function in the lifecycle:

[cols="1,2", options="header"]
|===
| Function | Components

| *Interface*
| Dashboard, Model Catalog

| *Development*
| Workbenches, AI Pipelines

| *Training*
| Distributed Workloads, Model Customization Suite

| *Serving*
| Single-Model Platform (KServe), Model Serving Runtimes

| *Monitoring*
| TrustyAI, Model Registry, Feature Store
|===

== Dashboard and Interface

=== OpenShift AI Dashboard

The dashboard is the central interface for both administrators and data scientists:

* *Administrators*: Manage users, configure cluster settings, control GPU allocation, approve workbench images
* *Data Scientists*: Create projects, launch workbenches, deploy models, access learning resources

The dashboard provides a unified view of all AI workloads running on the cluster.

=== Model Catalog

New in OpenShift AI 3.0, the *Model Catalog* is a curated library of generative AI models. Data scientists can:

* Browse models from Red Hat, IBM, Meta, NVIDIA, and other providers
* View model cards with capabilities, requirements, and intended use cases
* Deploy models directly to the serving platform with one click

The catalog uses OCI-based model URIs, meaning models are stored as container images for versioning and distribution:

[source,yaml]
----
modelUri: oci://registry.example.com/models/llama3-8b-instruct:latest
----

== Development and Training

=== Workbenches

Workbenches are isolated development environments for model experimentation. Each workbench is a container running an IDE:

[cols="1,1", options="header"]
|===
| Default IDE | Alternative Options

| *Jupyter* (self-managed)
| code-server, RStudio
|===

Workbenches include:

* Pre-installed ML libraries (PyTorch, TensorFlow, Transformers)
* GPU access for training and experimentation
* Persistent storage for notebooks and data
* Git integration for version control

==== Workbench Configuration with GPU Access

Workbenches use the `Notebook` Custom Resource Definition. Here's a configuration requesting GPU access:

[source,yaml]
----
apiVersion: kubeflow.org/v1
kind: Notebook
metadata:
  name: my-gpu-workbench
  namespace: my-project
  annotations:
    notebooks.opendatahub.io/inject-oauth: 'true'
    opendatahub.io/image-display-name: PyTorch
spec:
  template:
    spec:
      containers:
        - name: my-gpu-workbench
          image: 'image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/pytorch:2025.1'
          resources:
            limits:
              cpu: '2'
              memory: 8Gi
              nvidia.com/gpu: '1'  # Request 1 NVIDIA GPU
            requests:
              cpu: '1'
              memory: 8Gi
              nvidia.com/gpu: '1'
----

==== Available Workbench Images

OpenShift AI provides pre-built images optimized for different workloads:

[cols="1,2,1", options="header"]
|===
| Image | Includes | GPU Support

| *PyTorch*
| PyTorch, Python 3.11/3.12
| CUDA 12.6/12.8 or ROCm

| *TensorFlow*
| TensorFlow, Python 3.11/3.12
| CUDA 12.6/12.8 or ROCm

| *CUDA*
| NVIDIA CUDA Toolkit
| CUDA (compute-intensive)

| *ROCm*
| AMD ROCm libraries
| AMD GPU acceleration
|===

'''

*You might be wondering:* "_Why use a workbench instead of my local machine?_"

Workbenches provide consistent, reproducible environments with access to cluster resources (GPUs, storage, data) that your laptop can't match. They also ensure that experiments run in the same environment as production, reducing "works on my machine" issues.

'''

=== AI Pipelines

*AI Pipelines* (formerly Data Science Pipelines) automate ML workflows using portable, containerized steps. Based on *Kubeflow Pipelines 2.0*, they allow you to:

* Chain steps for data extraction, preprocessing, training, and validation
* Run pipelines on schedule or trigger-based
* Track experiments and compare runs
* Integrate with external systems

Each pipeline step runs in a Docker container, making workflows portable and reproducible.

[source,python]
----
# Example: Simple training pipeline
@component
def preprocess_data(data_path: str) -> str:
    # Load and clean data
    return processed_path

@component  
def train_model(data_path: str, epochs: int) -> str:
    # Train the model
    return model_path

@pipeline
def training_pipeline(raw_data: str):
    processed = preprocess_data(data_path=raw_data)
    model = train_model(data_path=processed.output, epochs=10)
----

=== Distributed Workloads

For models too large to train on a single GPU, OpenShift AI provides *distributed training* capabilities:

[cols="1,2", options="header"]
|===
| Component | Function

| *Ray* (KubeRay)
| Orchestrates distributed compute jobs across nodes

| *Kueue*
| Manages job queuing and quota enforcement

| *CodeFlare SDK*
| Python interface for defining distributed jobs

| *JobSet*
| Coordinates groups of jobs as single units

| *LeaderWorkerSet*
| Organizes pods into leader-worker patterns
|===

These components work together to split training across multiple GPUs and nodes, dramatically reducing training time.

'''

*You might be wondering:* "_How does distributed training actually speed things up?_"

Large models require more GPU memory than a single GPU provides. Distributed training splits the model and/or data across multiple GPUs:

* *Data parallelism*: Each GPU processes different batches of data, gradients are synchronized
* *Model parallelism*: Different model layers run on different GPUs
* *Pipeline parallelism*: Multiple micro-batches flow through different GPUs simultaneously

Kueue ensures fair resource allocation when multiple training jobs compete for GPUs.

'''

==== Kueue Configuration for GPU Quota Management

To manage GPU quotas across teams, you configure *ResourceFlavors* (defining infrastructure) and *ClusterQueues* (defining quotas).

*ResourceFlavor* — Defines a type of compute resource (e.g., NVIDIA GPU nodes):

[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: "nvidia-node"
spec:
  nodeLabels:
    instance-type: nvidia-node
  tolerations:
  - key: "HasGPU"
    operator: "Exists"
    effect: "NoSchedule"
----

*ClusterQueue* — Defines quotas for a team or project:

[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: "team-a-nvidia-queue"
spec:
  namespaceSelector: {}  # Matches all namespaces
  resourceGroups:
  - coveredResources: ["cpu", "memory", "nvidia.com/gpu"]
    flavors:
    - name: "nvidia-node"
      resources:
      - name: "cpu"
        nominalQuota: 16
      - name: "memory"
        nominalQuota: 64Gi
      - name: "nvidia.com/gpu"
        nominalQuota: 2  # Team limited to 2 GPUs
----

When a training job requests more GPUs than available, Kueue queues it until resources free up.

=== Model Customization Suite

New in OpenShift AI 3.0, the *Model Customization Suite* simplifies fine-tuning foundation models:

[cols="1,2", options="header"]
|===
| Component | Purpose

| *Training Hub*
| Framework for fine-tuning foundation models with simplified workflows

| *Kubeflow Trainer*
| Kubernetes-native distributed training operator

| *Synthetic Data Generation Hub*
| Toolkit for generating synthetic training data

| *Docling*
| Python library for processing unstructured documents
|===

==== Training Hub Workflow

Training Hub provides a Python SDK for submitting fine-tuning jobs. Here's a typical workflow:

[source,python]
----
from kubeflow.training import TrainingClient

# Authenticate to the cluster
client = TrainingClient(client_configuration=api_client.configuration)

# Create a distributed fine-tuning job
client.create_job(
    job_kind="PyTorchJob",
    name="llama-sft-fine-tuning",
    num_workers=8,
    resources_per_worker={"nvidia.com/gpu": 1, "memory": "64Gi"},
    base_image="quay.io/modh/training:py311-cuda124-torch251",
    parameters={
        "model_name_or_path": "Meta-Llama-3.1-8B-Instruct",
        "use_peft": "true",  # Use LoRA for efficient fine-tuning
        "num_train_epochs": "3"
    }
)
----

The SDK handles distributed coordination, checkpoint management, and resource scheduling automatically.

==== SDG Hub: Synthetic Data Generation

The *Synthetic Data Generation Hub* creates training data when real data is scarce or sensitive. It supports:

* *Knowledge Tuning*: Generating Q&A pairs from documents
* *Text Analysis*: Extracting structured insights from unstructured text

The SDG Hub integrates with *InstructLab* community tools, using the same underlying repository (`github.com/instructlab/sdg`) for data generation pipelines.

==== Docling: Document Processing for AI

Docling converts unstructured documents (PDFs, images, Word files) into structured, machine-readable formats:

[source,python]
----
from docling.document_converter import DocumentConverter

converter = DocumentConverter()
result = converter.convert("research_paper.pdf")

# Export as Markdown for LLM consumption
markdown_text = result.document.export_to_markdown()
----

This is essential for building RAG (Retrieval-Augmented Generation) systems where LLMs need to reference enterprise documents.

== Model Serving and Deployment

You learned about serving systems in Chapter 12. OpenShift AI provides managed serving platforms that implement those concepts.

=== Single-Model Serving Platform (KServe)

The *primary serving platform* for LLMs is based on KServe. Each model gets its own dedicated server—essential for large models requiring significant GPU resources.

*Why single-model for LLMs?*

* LLMs require dedicated GPU memory (often 16GB+ per model)
* KV cache grows with context length
* Models need independent scaling based on traffic
* Resource isolation prevents one model from starving another

=== Multi-Model Serving Platform (ModelMesh) — Deprecated

The *multi-model platform* allowed multiple models to share server resources. This was efficient for serving many small-to-medium models.

*Important:* ModelMesh is *deprecated in OpenShift AI 3.0*. Users should migrate to the single-model platform.

==== Migrating from ModelMesh to KServe

There is no automated migration tool. To migrate:

1. *Identify models*: List all models deployed on ModelMesh
2. *Create InferenceServices*: Redeploy each model using KServe `InferenceService` CRDs
3. *Update clients*: Point applications to new KServe endpoints
4. *Decommission*: Remove ModelMesh deployments after validation

For LLMs, this migration is straightforward since they should use single-model serving anyway.

'''

*You might be wondering:* "_Why deprecate multi-model serving? Isn't resource sharing more efficient?_"

For small traditional ML models (sklearn, XGBoost), sharing makes sense. But modern AI workloads are dominated by LLMs and other large models that:

1. Require dedicated GPU memory
2. Have unpredictable resource needs (KV cache growth)
3. Benefit from model-specific optimizations

The single-model architecture aligns with how LLMs actually work. KServe with RawDeployment mode provides efficiency without the complexity of ModelMesh.

'''

=== Deployment Modes

KServe supports two deployment modes with different trade-offs:

==== Serverless Mode (Default)

Uses *Knative* for automatic scaling, including scale-to-zero:

[cols="1,1", options="header"]
|===
| Pros | Cons

| Scale to zero when idle (cost savings)
| Requires OpenShift Serverless and Service Mesh

| Automatic scaling on demand
| Cold start latency when scaling from zero

| Traffic-based autoscaling
| Larger infrastructure footprint
|===

*Dependencies:*

* Red Hat OpenShift Serverless (Knative)
* Red Hat OpenShift Service Mesh (Istio)
* cert-manager

==== RawDeployment Mode

Uses standard Kubernetes resources (Deployment, Service, HPA):

[cols="1,1", options="header"]
|===
| Pros | Cons

| No additional dependencies
| No scale-to-zero

| Smaller resource footprint
| Must manage scaling manually or via KEDA

| Full control over pod specs
| Always running (cost when idle)

| Simpler debugging
|
|===

*When to use RawDeployment:*

* Models that must remain available continuously
* Custom serving configurations
* Environments where Serverless adds unwanted complexity
* When you want predictable latency (no cold starts)

==== Autoscaling with KEDA

For RawDeployment mode, *KEDA* (Kubernetes Event-Driven Autoscaling) triggers scaling based on metrics. For LLM workloads, the most effective metrics are:

[cols="1,2", options="header"]
|===
| Metric | Best For

| `vllm:num_requests_waiting`
| Throughput optimization (scale when queue grows)

| Time to First Token (TTFT)
| Latency-sensitive workloads

| Inter-Token Latency (ITL)
| Variable sequence length workloads
|===

Example InferenceService with autoscaling:

[source,yaml]
----
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 5
    autoscaling:
      metrics:
        - type: External
          external:
            metric:
              backend: "prometheus"
              query: vllm:num_requests_waiting
          target:
            type: Value
            value: 2  # Scale up when >2 requests queued
----

=== Summary: Deployment Mode Requirements

[cols="1,1,1,1", options="header"]
|===
| Requirement | Serverless Mode | RawDeployment Mode | ModelMesh (Deprecated)

| OpenShift Serverless
| Required
| Not Required
| Not Required

| OpenShift Service Mesh
| Required
| Not Required
| Not Required

| cert-manager
| Required
| Required
| Optional

| Scale-to-Zero
| Supported
| Not Supported
| Not Supported

| Primary Use Case
| Cost optimization via scaling
| Always-on models
| Small/medium models sharing resources
|===

=== Authentication for Model Endpoints

When exposing models externally, you need to secure the endpoints. OpenShift AI uses *Authorino* for token-based authentication with KServe.

Configure an `AuthorizationPolicy` to protect predictor pods:

[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: kserve-predictor
spec:
  action: CUSTOM
  provider:
    name: redhat-ods-applications-auth-provider
  rules:
    - to:
        - operation:
            notPaths:
              - /healthz
              - /metrics
  selector:
    matchLabels:
      component: predictor
----

This policy requires authentication for all inference requests while allowing unauthenticated access to health checks and metrics.

=== Multi-GPU Model Deployment

For large models that don't fit on a single GPU, vLLM supports *tensor parallelism* and *pipeline parallelism*.

[cols="1,2,1", options="header"]
|===
| Parallelism Type | What It Does | Configuration

| *Tensor Parallelism*
| Splits model layers across GPUs on one node
| `tensorParallelSize: 4`

| *Pipeline Parallelism*
| Distributes layers across multiple nodes
| `pipelineParallelSize: 2`
|===

Configure in the InferenceService `workerSpec` or via runtime arguments:

[source,yaml]
----
spec:
  predictor:
    model:
      runtime: vllm-cuda-runtime
      args:
        - --tensor-parallel-size=4  # Use 4 GPUs per node
----

For an 8-GPU deployment across 2 nodes:

* `tensorParallelSize: 4` (4 GPUs per node)
* `pipelineParallelSize: 2` (2 nodes)

=== NVIDIA NIM Model Serving

For NVIDIA-optimized inference, OpenShift AI supports *NVIDIA Inference Microservices (NIM)*. NIM provides:

* Pre-optimized containers for NVIDIA hardware
* TensorRT-LLM optimizations baked in
* Easy deployment of NVIDIA-hosted models

This complements the vLLM-based serving runtime for users with specific NVIDIA optimization requirements.

== Multi-Modal Deployment

Modern AI applications often require *vision-language models (VLMs)* that process both text and images.

=== Enabling Multi-Modal Inference

OpenShift AI supports VLMs through the vLLM runtime with specific configuration:

1. Use the *vLLM NVIDIA GPU ServingRuntime for KServe*
2. Add `--trust-remote-code` to serving runtime arguments (required for many VLMs)

[source,yaml]
----
spec:
  containers:
  - name: kserve-container
    args:
    - --trust-remote-code  # Required for VLMs
    - --model=/mnt/models
----

*Security Note:* Only use `--trust-remote-code` with models from trusted sources.

=== Multi-Modal API Structure

Requests to VLMs include both text and image content:

[source,json]
----
{
  "model": "llava-1.5-7b",
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "text", "text": "What's in this image?"},
        {"type": "image_url", "image_url": "https://example.com/photo.jpg"}
      ]
    }
  ]
}
----

The model processes both modalities to generate a text response describing or analyzing the image.

== Management and Monitoring

=== Model Registry

The *Model Registry* is a central repository for tracking AI models throughout their lifecycle:

* *Version Control*: Track model versions with metadata
* *Lineage*: Record which data and code produced each model
* *Deployment Tracking*: Know which models are deployed where
* *Collaboration*: Share models across teams

Based on *Kubeflow Model Registry*, it provides the governance layer for enterprise AI.

=== TrustyAI

*TrustyAI* is a responsible AI toolkit for ensuring models are fair and reliable:

[cols="1,2", options="header"]
|===
| Capability | Purpose

| *Bias Detection*
| Identify unfair patterns in predictions

| *Data Drift Monitoring*
| Detect when input data distribution changes

| *Explainability*
| Understand why models make specific predictions
|===

This is critical for regulated industries where AI decisions must be auditable.

==== TrustyAI Configuration

Deploy TrustyAI by creating a `TrustyAIService` Custom Resource:

[source,yaml]
----
apiVersion: trustyai.opendatahub.io/v1
kind: TrustyAIService
metadata:
  name: trustyai-service
spec:
  storage:
    format: "DATABASE"
    size: "1Gi"
    databaseConfigurations: db-credentials  # Secret with DB connection
  metrics:
    schedule: "5s"  # Collect metrics every 5 seconds
----

*Integration Workflow:*

1. Authenticate via the OAuth proxy to obtain a token
2. Upload training data to the `/data/upload` endpoint
3. Register the deployed model's InferenceService with TrustyAI
4. TrustyAI begins monitoring predictions for bias and drift

==== LLM-Specific Prometheus Metrics

OpenShift AI exposes detailed metrics for vLLM-based inference:

[cols="1,2", options="header"]
|===
| Metric | Description

| `vllm:time_to_first_token_seconds`
| TTFT — time until first token generated

| `vllm:time_per_output_token_seconds`
| TPOT — average time between tokens

| `vllm:generation_tokens_total`
| Total tokens generated (for throughput)

| `vllm:num_requests_waiting`
| Requests queued (for scaling decisions)

| `vllm:gpu_cache_usage_perc`
| KV cache utilization percentage

| `request_success_total`
| Successful requests (for availability)
|===

These metrics integrate with Prometheus and can trigger KEDA autoscaling or Grafana alerts.

=== Feature Store (Technology Preview)

Based on *Feast*, the Feature Store is a centralized repository for storing, managing, and serving ML features. It solves a critical problem: ensuring the features used during training match those used during inference.

==== Feature Store Architecture

[cols="1,2", options="header"]
|===
| Component | Purpose

| *Registry*
| Central catalog of feature definitions and metadata

| *Offline Store*
| Historical time-series data for batch training (S3, Snowflake, BigQuery)

| *Online Store*
| Low-latency store for real-time inference (Redis, PostgreSQL, DynamoDB)

| *Feature Server*
| HTTP/gRPC endpoint serving pre-computed features
|===

==== Key Concepts

* *Feature*: An individual measurable property (e.g., `credit_score`, `purchase_history`)
* *Entity*: A primary key grouping related features (e.g., `customer_id`)
* *Feature View*: A logical group of features from a specific data source

==== Supported Data Stores

[cols="1,2", options="header"]
|===
| Component | Supported Options

| *Offline Store*
| Dask, Snowflake, BigQuery, Redshift, DuckDB

| *Online Store*
| Redis, PostgreSQL, DynamoDB, Snowflake, SQLite, Cassandra, MySQL

| *Registry*
| S3, Google Cloud Storage (GCS), SQL, Snowflake
|===

The Feature Store ensures *point-in-time correctness*—training datasets use data that was accurate at the specific time of the event, preventing data leakage that could inflate model performance metrics.

== Retrieval-Augmented Generation (RAG)

RAG combines LLM generation with document retrieval. OpenShift AI supports RAG through the *Llama Stack Operator* (Technology Preview):

[cols="1,2", options="header"]
|===
| Component | Role

| *Llama Stack*
| Unified AI runtime for LLM inference

| *Vector Database*
| Stores document embeddings (e.g., Pgvector)

| *Retrieval Service*
| Finds relevant documents for each query

| *Ragas*
| Evaluates RAG pipeline quality
|===

RAG workflow:

1. User asks a question
2. Retrieval service finds relevant documents
3. Documents are added to the LLM context
4. LLM generates an answer grounded in retrieved content

This reduces hallucinations by providing factual context.

=== Llama Stack Vector Database Integration

Llama Stack connects to vector databases via the `LlamaStackDistribution` Custom Resource.

*Connecting to Remote Milvus:*

Create a secret with connection details:

[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: milvus-credentials
stringData:
  MILVUS_ENDPOINT: "tcp://milvus-service:19530"
  MILVUS_TOKEN: "your-auth-token"
----

Reference the secret in the LlamaStackDistribution:

[source,yaml]
----
apiVersion: llamastack.io/v1
kind: LlamaStackDistribution
metadata:
  name: rag-stack
spec:
  vectorStore:
    type: milvus
    credentialsSecretRef:
      name: milvus-credentials
----

*Embedded FAISS Option:*

For simpler deployments, use an embedded SQLite backend with FAISS:

[source,yaml]
----
spec:
  vectorStore:
    type: faiss
    environment:
      ENABLE_FAISS: "faiss"
----

Docling can prepare documents for ingestion into these vector stores by converting PDFs and other formats into structured text suitable for embedding.

== The Open Source Foundation

OpenShift AI is built on open source projects. Understanding these helps you troubleshoot and extend the platform.

=== Model Serving

[cols="1,2", options="header"]
|===
| Project | Role in OpenShift AI

| *KServe*
| Single-model serving orchestration

| *Knative*
| Serverless deployment (scale-to-zero)

| *Istio*
| Service mesh for traffic and security

| *vLLM*
| High-throughput LLM inference engine

| *OpenVINO*
| Intel-optimized inference

| *ModelMesh*
| Multi-model serving (deprecated)
|===

=== Training and Pipelines

[cols="1,2", options="header"]
|===
| Project | Role in OpenShift AI

| *Kubeflow Pipelines*
| ML workflow orchestration

| *Argo Workflows*
| Pipeline execution engine

| *Kueue*
| Job queuing and quota management

| *Ray (KubeRay)*
| Distributed compute orchestration

| *Kubeflow Training Operator*
| Distributed training for PyTorch, TensorFlow

| *Elyra*
| Visual pipeline editor for Jupyter
|===

=== Data and Features

[cols="1,2", options="header"]
|===
| Project | Role in OpenShift AI

| *Feast*
| Feature store

| *Kubeflow Model Registry*
| Model versioning and tracking
|===

=== Monitoring and Observability

[cols="1,2", options="header"]
|===
| Project | Role in OpenShift AI

| *TrustyAI*
| Bias detection and explainability

| *Prometheus*
| Metrics collection

| *OpenTelemetry*
| Telemetry data standardization

| *Tempo*
| Distributed tracing
|===

=== Generative AI

[cols="1,2", options="header"]
|===
| Project | Role in OpenShift AI

| *Llama Stack*
| Unified GenAI runtime (RAG, agents)

| *Docling*
| Document processing

| *Ragas*
| RAG pipeline evaluation
|===

== Infrastructure Dependencies

OpenShift AI requires additional operators for full functionality:

[cols="1,2,2", options="header"]
|===
| Operator | Purpose | Required For

| *OpenShift Serverless*
| Knative-based serverless
| KServe Serverless mode

| *OpenShift Service Mesh*
| Istio-based networking
| KServe Serverless mode

| *cert-manager*
| Certificate lifecycle
| Secure endpoints

| *Node Feature Discovery*
| Hardware detection
| GPU scheduling

| *NVIDIA GPU Operator*
| NVIDIA GPU support
| NVIDIA inference

| *AMD GPU Operator*
| AMD GPU support
| AMD inference

| *Intel Gaudi Base Operator*
| Intel Gaudi accelerator support
| Intel Gaudi inference
|===

For RawDeployment mode, only cert-manager is strictly required.

=== Intel Gaudi Support

Beyond NVIDIA and AMD, OpenShift AI supports *Intel Gaudi AI accelerators*. Configuration differs from NVIDIA:

[cols="1,1,1", options="header"]
|===
| Aspect | NVIDIA | Intel Gaudi

| *Operator*
| NVIDIA GPU Operator
| Intel Gaudi Base Operator

| *Workbench Images*
| Pre-built images available
| Custom image required

| *Kubelet Config*
| Default
| Increase pod PID limit to 32,768
|===

The higher PID limit prevents resource exhaustion errors specific to Gaudi workloads. Configure via `KubeletConfig`:

[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: gaudi-pid-limit
spec:
  kubeletConfig:
    podPidsLimit: 32768
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: ""
----

== Putting It All Together

Here's how the components connect in a typical LLM workflow:

[source]
----
┌─────────────────────────────────────────────────────────────────┐
│                     OpenShift AI Dashboard                       │
│                  (Interface for all operations)                  │
└─────────────────────────────────────────────────────────────────┘
                                │
        ┌───────────────────────┼───────────────────────┐
        │                       │                       │
        ▼                       ▼                       ▼
┌───────────────┐     ┌─────────────────┐     ┌─────────────────┐
│  Development  │     │    Training     │     │    Serving      │
│               │     │                 │     │                 │
│  Workbenches  │────▶│  Kueue + Ray    │────▶│  KServe         │
│  (Jupyter)    │     │  (Distributed)  │     │  (vLLM runtime) │
│               │     │                 │     │                 │
│  AI Pipelines │     │  Training Hub   │     │  Model Catalog  │
│  (Kubeflow)   │     │  (Fine-tuning)  │     │  (OCI models)   │
└───────────────┘     └─────────────────┘     └─────────────────┘
                                │                       │
                                ▼                       ▼
                      ┌─────────────────┐     ┌─────────────────┐
                      │  Model Registry │     │   TrustyAI      │
                      │  (Versioning)   │     │   (Monitoring)  │
                      └─────────────────┘     └─────────────────┘
----

A data scientist might:

1. *Develop* in a Jupyter workbench, experimenting with prompts and fine-tuning approaches
2. *Automate* training with AI Pipelines, running distributed jobs via Kueue and Ray
3. *Register* the trained model in Model Registry with metadata
4. *Deploy* through the Dashboard, selecting a model from the Catalog
5. *Monitor* with TrustyAI, watching for bias and drift

== Chapter Takeaway

[NOTE]
====
OpenShift AI unifies the complete AI/ML lifecycle on Kubernetes. The platform combines open source projects (KServe, vLLM, Kubeflow, Ray) with enterprise management (Dashboard, Model Registry, TrustyAI) to provide development, training, serving, and monitoring in a single integrated environment. For LLMs, the single-model serving platform based on KServe provides the dedicated resources and GPU isolation that large models require.
====

'''

_Next: xref:14-vocabulary.adoc[Chapter 14: The Complete Vocabulary]_
