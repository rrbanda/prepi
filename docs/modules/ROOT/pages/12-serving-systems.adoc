= Chapter 12: Serving Systems — vLLM and Friends
:description: Understanding model serving systems that make LLM deployment practical at scale

_Part VI: Under the Hood_

'''

You've learned how a single request works. But what about serving millions of users simultaneously?

That's where *serving systems* come in — software that optimizes LLM deployment for production workloads.

== The Production Challenge

A single user making one request is straightforward. But real deployments face:

* *Thousands of concurrent requests*
* *Limited GPU memory* (expensive!)
* *Variable request lengths* (some short, some long)
* *Latency requirements* (users expect fast responses)
* *Cost pressure* (GPU time is expensive)

Naive deployment wastes resources. Serving systems fix this.

== What Serving Systems Optimize

=== 1. KV Cache Management

The KV cache (Chapter 11) can consume massive amounts of memory. Serving systems manage this carefully:

* *Efficient allocation*: Don't waste memory on padding
* *Memory sharing*: Reuse memory across requests when possible
* *Preemption*: Pause low-priority requests if memory is needed

=== 2. Batching

Instead of processing one request at a time:

[source]
----
Request A → Process → Done
Request B → Process → Done
Request C → Process → Done
----

Process multiple requests together:

[source]
----
[Request A, B, C] → Process together → [Done, Done, Done]
----

GPUs excel at parallel computation. Batching uses this to serve more users with the same hardware.

=== 3. Continuous Batching

The problem with simple batching: requests have different lengths.

[source]
----
Request A: 10 tokens (done quickly)
Request B: 100 tokens (still generating)
Request C: 5 tokens (done even quicker)
----

Continuous batching dynamically adds and removes requests from the batch as they complete:

[source]
----
Batch starts: [A, B, C]
A completes: [B, C, D]  ← D joins
C completes: [B, D, E]  ← E joins
B completes: [D, E, F]  ← F joins
----

No waiting. Maximum GPU utilization.

==== Continuous Batching Step-by-Step

Let's trace through exactly what happens at each decoding step:

[cols="1,2,1", options="header"]
|===
|Decoding Step
|What Happens
|Active Requests

|Step 1
|Generate token 1 for requests A, B, C
|A, B, C

|Step 2
|Request D arrives → added to batch
|A, B, C, D

|Step 3
|Generate: token 2 for A,B,C + token 1 for D
|A, B, C, D

|Step 4
|A completes (hit stop token) → removed
|B, C, D

|Step 5
|Request E arrives → added
|B, C, D, E

|Step 6
|Generate: token 3 for B,C + token 2 for D + token 1 for E
|B, C, D, E
|===

Key insight: *Every request is treated independently, even in the same batch.* Requests can have different prompt sizes and generate different numbers of tokens. The system handles this naturally.

'''

*You might be wondering:* "_How can requests with different prompt lengths and generation positions be processed together in the same batch? Doesn't batching require identical shapes?_"

The system uses padding and attention masking. Shorter sequences are padded to match the longest in the batch, and attention masks ensure each request only attends to its own valid tokens (ignoring padding). Each request also tracks its own position and generation state independently. The GPU processes the padded batch in parallel, but the masking makes each request's computation independent. This is why variable-length batching works without corrupting outputs.

'''

==== Why Continuous Batching Matters

[cols="1,1", options="header"]
|===
|Metric
|Static vs Continuous

|GPU utilization
|Low (waiting) vs High (always processing)

|Latency
|High (blocked) vs Low (immediate)

|Throughput
|Limited vs Maximized

|Memory efficiency
|Poor vs Optimized
|===

This is the default behavior in vLLM — you get it automatically.

== The Scheduler: Heart of the Serving System

The *scheduler* is the central coordinator that makes continuous batching work. It's the brain of the inference engine.

=== What the Scheduler Does

[source]
----
┌─────────────────────────────────────────────────────────┐
│                      SCHEDULER                          │
│                                                         │
│   ┌───────────┐    ┌───────────┐    ┌───────────┐      │
│   │  WAITING  │───▶│  RUNNING  │───▶│ COMPLETED │      │
│   │   Queue   │    │   Queue   │    │           │      │
│   └───────────┘    └─────┬─────┘    └───────────┘      │
│         ▲                │                              │
│         │                ▼                              │
│         │          ┌───────────┐                        │
│         └──────────│  SWAPPED  │                        │
│                    │   Queue   │                        │
│                    └───────────┘                        │
└─────────────────────────────────────────────────────────┘
----

=== Request Lifecycle States

[cols="1,2,1", options="header"]
|===
|State
|Description
|When It Happens

|*Waiting*
|Request received, not yet processing
|New requests enter here

|*Running*
|Actively generating tokens
|Scheduler picked this request

|*Swapped*
|Paused, KV cache moved to CPU
|Memory pressure, preemption

|*Completed*
|Finished generating
|Hit stop token or max length
|===

=== Core Scheduler Responsibilities

[cols="1,2", options="header"]
|===
|Component
|Description

|`schedule()`
|Entry point — decides what requests run next

|Queue management
|Tracks waiting, running, and swapped requests

|Token allocation
|Determines how many tokens each request gets per step

|Block management
|Coordinates with BlockSpaceManager for KV cache

|Preemption logic
|Decides when to pause requests to free memory

|Speculative coordination
|Manages multi-step decoding when enabled
|===

=== Preemption: Handling Memory Pressure

When GPU memory fills up, the scheduler must make hard choices:

[source]
----
Scenario: GPU memory at 95% capacity
          New high-priority request arrives
          
Options:
1. SWAP: Move a running request's KV cache to CPU RAM
         - Slower but preserves progress
         - Request can resume later
         
2. RECOMPUTE: Evict KV cache entirely
              - Request restarts from prompt
              - Wastes prior computation
              - Frees more memory
----

The scheduler uses policies to decide:

* *FIFO*: First-in-first-out eviction
* *Priority-based*: Low-priority requests evicted first
* *Size-based*: Large KV caches evicted first

=== Why Understanding the Scheduler Matters

When debugging production issues:

[cols="1,1,1", options="header"]
|===
|Symptom
|Likely Cause
|Scheduler-Related Fix

|High latency spikes
|Queue buildup
|Increase `max_num_seqs`

|OOM errors
|Too many running requests
|Reduce `max_num_batched_tokens`

|Inconsistent latency
|Preemption happening
|Increase GPU memory or reduce concurrency

|Low throughput
|Conservative scheduling
|Tune `gpu-memory-utilization` higher
|===

== vLLM: PagedAttention

*vLLM* is a popular open-source serving system. Its key innovation is *PagedAttention*.

=== The Problem: Memory Fragmentation

Traditional KV cache allocation:

[source]
----
Request A needs space for 1000 tokens → allocate contiguous 1000-token block
Request B needs space for 500 tokens  → allocate contiguous 500-token block
Request C needs space for 2000 tokens → allocate contiguous 2000-token block

What if Request B finishes first?
→ 500-token hole in memory
→ Request D needs 800 tokens → doesn't fit in the hole!
→ Memory fragmented, wasted space
----

=== The Solution: Paged Memory

vLLM treats KV cache like operating system virtual memory:

[source]
----
Divide memory into small pages (e.g., 16 tokens each)

Request A: needs 1000 tokens → allocate 63 pages (non-contiguous OK!)
Request B: needs 500 tokens  → allocate 32 pages
Request C: needs 2000 tokens → allocate 125 pages

Request B finishes → 32 pages freed
Request D needs 800 tokens → gets 50 pages, using B's freed pages + new ones

No fragmentation!
----

Pages can be anywhere in memory. A request's pages don't need to be contiguous.

This simple insight dramatically increases how many concurrent requests can run.

'''

*You might be wondering:* "_What's the difference between PagedAttention and BlockSpaceManager? Are they the same thing?_"

PagedAttention is the algorithm/concept: the idea of dividing KV cache memory into pages (like OS virtual memory) to avoid fragmentation. BlockSpaceManager is the implementation: the actual code component that manages page allocation, logical-to-physical mapping, eviction, and reuse. Think of PagedAttention as the design pattern and BlockSpaceManager as the engine that implements it. When you read about "PagedAttention" in research papers, it refers to the technique; in vLLM's codebase, BlockSpaceManager is the component you'd examine.

'''

== Speculative Decoding: Thinking Ahead

*Speculative decoding* is a powerful optimization technique that can dramatically reduce inference latency without changing the model's outputs.

=== The Core Idea

Instead of generating one token at a time with your large model:

. Use a *small, fast "draft" model* to predict several tokens ahead
. Have the *large "target" model* verify those predictions in parallel
. Accept correct predictions, reject wrong ones
. Net result: fewer expensive forward passes through the large model

[source]
----
Traditional Decoding:
Token 1 → [Large Model] → Token 2 → [Large Model] → Token 3 → ...
          (slow)                    (slow)

Speculative Decoding:
[Draft Model] → Speculate: Token 2, 3, 4, 5
(fast)                    ↓
[Target Model] → Verify all 4 in ONE forward pass
(expensive but done once)
                          ↓
Accept: 2, 3, 4 ✓  Reject: 5 ✗
→ Generated 3 tokens with 1 expensive pass!
----

=== Benefits

[cols="1,2", options="header"]
|===
|Benefit
|Explanation

|*Lower latency*
|Fewer expensive forward passes through large model

|*Same output quality*
|Target model still makes final decisions

|*No retraining needed*
|Drop-in optimization

|*Model-agnostic*
|Works with any transformer decoder
|===

=== Trade-offs

[cols="1,2", options="header"]
|===
|Challenge
|Details

|*Extra GPU memory*
|Both draft and target model must be loaded

|*Acceptance rate varies*
|Depends on draft model quality and sampling strategy

|*Implementation complexity*
|Two models must coordinate

|*Not always faster*
|Short sequences or high randomness may not benefit
|===

=== When Speculative Decoding Helps

[cols="1,1", options="header"]
|===
|Scenario
|Benefit Level

|Long generations (100+ tokens)
|*High* — amortizes setup cost

|Greedy/low-temperature sampling
|*High* — predictable, high acceptance

|Code generation
|*High* — structured, predictable patterns

|Short responses (<20 tokens)
|*Low* — overhead exceeds benefit

|High temperature/creative writing
|*Low* — randomness kills acceptance rate

|Beam search
|*Not compatible*
|===

=== vLLM Speculative Decoding

vLLM supports speculative decoding out of the box:

[source,bash]
----
# Enable speculative decoding with a draft model
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3.2-70B \
    --speculative-model meta-llama/Llama-3.2-7B \
    --num-speculative-tokens 4
----

Key parameters:

* `--speculative-model`: Path to the draft model
* `--num-speculative-tokens`: How many tokens to speculate (typically 3-5)

'''

*You might be wondering:* "_When should I NOT use speculative decoding?_"

Avoid speculative decoding when: (1) output is very short (<20 tokens) — the overhead of running two models exceeds any speedup; (2) sampling is highly random (high temperature) — low acceptance rate means wasted computation on rejected speculation; (3) you're using beam search — incompatible decoding approach; or (4) you can't fit both models in GPU memory — no room for a draft model. It works best for long, predictable outputs like code generation, documentation, or structured data extraction.

'''

== Other Serving Systems

vLLM isn't the only option:

[cols="1,1", options="header"]
|===
|System
|Key Feature

|*vLLM*
|PagedAttention, continuous batching

|*TensorRT-LLM*
|NVIDIA optimization, custom kernels

|*Text Generation Inference (TGI)*
|HuggingFace, easy to use

|*Triton Inference Server*
|NVIDIA, production-focused

|*DeepSpeed-Inference*
|Microsoft, large model support
|===

Different trade-offs: ease of use vs. raw performance, open-source vs. vendor-supported.

=== Competitive Landscape: Understanding the Alternatives

==== NVIDIA TensorRT-LLM (TRT-LLM)

TensorRT-LLM is NVIDIA's optimized inference engine for LLMs.

[cols="1,2", options="header"]
|===
|Aspect
|Details

|*Hardware*
|NVIDIA GPUs only — no AMD, Intel, or CPU fallback

|*Optimization approach*
|Low-level graph optimizations, kernel fusion, quantization

|*Strengths*
|Maximum performance on NVIDIA hardware

|*Weaknesses*
|Vendor lock-in, less flexible, steeper learning curve
|===

==== NVIDIA NIM

NIM (NVIDIA Inference Microservices) is NVIDIA's containerized model-serving solution.

[cols="1,2", options="header"]
|===
|Aspect
|Details

|*What it is*
|Pre-packaged containers for specific models (LLaMA, Mistral, etc.)

|*Backend*
|Uses TRT-LLM or vLLM depending on model and hardware

|*Strengths*
|Simple to deploy, OpenAI-compatible API

|*Weaknesses*
|NVIDIA-only, less customization, closed source
|===

==== SGLang

SGLang is a fast, open-source serving framework with a programming focus.

[cols="1,2", options="header"]
|===
|Aspect
|Details

|*Focus*
|Developer-friendly programming interface for LLM workflows

|*Key innovation*
|RadixAttention for prefix caching, structured output support

|*Strengths*
|Excellent for agents, function-calling, complex pipelines

|*Weaknesses*
|Smaller community than vLLM
|===

== Quantization: Making Models Smaller

*Quantization* reduces the precision of model weights, trading some accuracy for dramatic improvements in memory usage and inference speed.

=== Why Quantization Matters

A typical LLM weight is stored in FP16 (16-bit floating point). Quantization converts these to lower precision:

[cols="1,1,1,1", options="header"]
|===
|Precision
|Bits per Weight
|Memory Reduction
|Speed Impact

|FP32
|32 bits
|Baseline
|Slow

|FP16/BF16
|16 bits
|2× smaller
|Standard

|INT8
|8 bits
|4× smaller
|Faster

|INT4
|4 bits
|8× smaller
|Much faster
|===

=== Quantization Formats for vLLM

[cols="1,2,2", options="header"]
|===
|Format
|Description
|Best Use Case

|*W4A16*
|4-bit weights, FP16 activations
|Memory-constrained, edge, containerized apps

|*W8A8-INT8*
|8-bit weights, INT8 activations
|High-throughput, general purpose, any GPU

|*W8A8-FP8*
|8-bit weights, FP8 activations
|Accuracy-sensitive + memory constraints, H100 GPUs

|*2:4 Sparsity + FP8*
|Structured sparsity with FP8
|Maximum speed on H100/Blackwell
|===

=== Quantization Tools

==== llm-compressor (Red Hat/Neural Magic)

The recommended tool for quantizing models for vLLM deployment:

[source,python]
----
from llmcompressor import compress
from llmcompressor.modifiers import SmoothQuantModifier, GPTQModifier

# Quantize a model to W4A16
recipe = [
    SmoothQuantModifier(smoothing_strength=0.5),
    GPTQModifier(scheme="W4A16", ignore=["lm_head"]),
]

compressed_model = compress(
    model="meta-llama/Llama-3.2-7B",
    recipe=recipe,
    calibration_data="calibration_dataset"
)
----

==== Other Tools

* *AutoAWQ*: Activation-aware Weight Quantization
* *bitsandbytes*: 8-bit and 4-bit quantization
* *GGUF*: Single-file quantized format (vLLM supports single-file only)

=== Accuracy Impact

Well-quantized models typically recover *99%+ of baseline accuracy*:

[cols="1,1,1,1", options="header"]
|===
|Model
|Baseline (FP16)
|W8A8-INT8
|W4A16

|LLaMA 7B
|100%
|99.5%
|98.8%

|Mistral 7B
|100%
|99.6%
|99.1%

|LLaMA 70B
|100%
|99.7%
|99.3%
|===

_Accuracy measured on standard benchmarks (MMLU, HellaSwag, etc.)_

== What They DON'T Change

Here's the important point: *Serving systems optimize how models run, not what models do.*

They do NOT change:

* The model architecture
* The model weights
* The attention math
* What the model outputs

They DO change:

* How fast you get the output
* How many users you can serve
* How much GPU memory you need
* How much it costs

Think of serving systems as *traffic management for GPUs*. They don't make the cars faster — they make the highway more efficient.

== When to Care About This

[cols="1,1", options="header"]
|===
|Situation
|Do You Need This Knowledge?

|Using ChatGPT API
|No — handled for you

|Building a chatbot product
|Maybe — for cost optimization

|Deploying your own models
|Yes — critical for performance

|Working on ML/AI infrastructure
|Yes — shows systems understanding

|ML research
|Maybe — depends on focus
|===

If you're just using LLMs through APIs, you can treat this as a black box. If you're deploying or optimizing, this is essential.

== Production Metrics Deep Dive

Understanding metrics is essential for operating LLMs in production. Different use cases prioritize different metrics.

=== Latency Metrics

==== Time to First Token (TTFT)

*Definition*: Time from request arrival to first token returned.

*Why it matters*:

* Most user-visible latency metric
* Critical for chatbots, copilots, interactive tools
* High TTFT makes applications feel sluggish

*What affects TTFT*:

* Prompt length (longer prompts = higher TTFT)
* Queue depth (more waiting requests = higher TTFT)
* Model size
* Prefix caching (can reduce TTFT significantly)

==== Time Per Output Token (TPOT)

*Definition*: Average time between consecutive generated tokens.

*Why it matters*:

* Determines streaming "smoothness"
* Critical for code assistants, real-time summarization
* Users perceive smooth streaming as faster

==== Inter-Token Latency (ITL)

*Definition*: Time between each subsequent token during streaming.

High ITL variance causes "stuttering" in streamed responses.

==== End-to-End Request Latency

*Definition*: Total time from request to complete response.

This includes queue waiting, prompt processing, and all token generation.

=== Throughput Metrics

==== Tokens Per Second (TPS)

*Definition*: Total output tokens generated per second across all requests.

*Why it matters*:

* Raw capacity of the system
* Higher TPS = more users served
* Lower infrastructure cost per token

==== Goodput

*Definition*: Requests per second that meet Service Level Objectives (SLOs).

Example SLOs:

* TTFT < 500ms at P90
* TPOT < 50ms at P90
* End-to-end < 10s at P99

*Why it matters*:

* Aligns infrastructure performance with user-facing requirements
* Throughput alone doesn't guarantee good user experience

==== P95/P99 Latency

*Definition*: Latency at the 95th/99th percentile.

*Why it matters*:

* Enterprise SLAs often defined at P95/P99
* Mean latency hides outliers
* Users remember the worst experiences

=== Metric Optimization by Use Case

[cols="1,2,2", options="header"]
|===
|Use Case
|Primary Metrics
|Secondary Metrics

|*Chatbots*
|TTFT, ITL
|End-to-end latency

|*Code assistants*
|TTFT, TPOT
|Accuracy

|*Document processing*
|Throughput (TPS)
|Cost per token

|*RAG applications*
|TTFT, concurrency
|Memory utilization

|*LLM-as-a-Service*
|P95 latency, throughput
|Cost, availability
|===

== Evaluation and Benchmarking Tools

Measuring LLM performance requires specialized tools.

=== GuideLLM

*GuideLLM* is a benchmarking framework designed for evaluating LLM inference performance.

==== What It Measures

* TTFT (mean, median, P99)
* TPOT, ITL, request latency
* Requests/second, tokens/second
* Concurrency

==== Running a Benchmark

[source,bash]
----
# Install
pip install guidellm

# Run benchmark
guidellm \
    --target "http://localhost:8000/v1" \
    --model "meta-llama/Llama-3.2-8B" \
    --data-type emulated \
    --data "prompt_tokens=512,generated_tokens=128"
----

GuideLLM runs a *sweep* across multiple load levels, helping you find the optimal operating point for your system.

=== lm-eval-harness

*lm-eval-harness* is the standard framework for evaluating model accuracy on benchmarks.

==== What It Measures

* General knowledge: MMLU, HellaSwag, ARC, TruthfulQA
* Reasoning: GSM8K, MATH, BIG-Bench
* Code: HumanEval, MBPP

==== Running Evaluations

[source,bash]
----
# Install
pip install lm-eval

# Evaluate a model on MMLU
lm_eval --model vllm \
    --model_args pretrained=meta-llama/Llama-3.2-8B \
    --tasks mmlu \
    --batch_size auto
----

=== Ragas

*Ragas* (Retrieval Augmented Generation Assessment) evaluates RAG pipeline quality.

==== What It Measures

* *Faithfulness*: Is the answer grounded in retrieved context?
* *Answer relevancy*: Does the answer address the question?
* *Context precision*: Are retrieved documents relevant?
* *Context recall*: Did retrieval find all relevant information?

=== Choosing the Right Tool

[cols="1,1", options="header"]
|===
|Goal
|Tool

|Benchmark inference speed
|GuideLLM

|Measure model accuracy
|lm-eval-harness

|Evaluate RAG quality
|Ragas

|Production monitoring
|Prometheus + Grafana with vLLM metrics
|===

== Key Metrics Summary

[cols="1,2", options="header"]
|===
|Metric
|Definition

|*TTFT*
|Time To First Token — how fast the response starts

|*TPOT*
|Time Per Output Token — streaming speed

|*TPS*
|Tokens Per Second — system throughput

|*Goodput*
|Requests/sec meeting SLOs

|*P95/P99*
|Tail latency for SLA compliance

|*Memory utilization*
|How efficiently GPU memory is used

|*Cost per token*
|Infrastructure cost per generated token
|===

Serving systems optimize these metrics simultaneously.

== Pause and Reflect

Here's a mental model:

* *The model* is like a factory's machinery — the actual production capability
* *Serving systems* are like factory logistics — scheduling, resource allocation, throughput optimization

You can have great machinery but terrible logistics (inefficient). Or great logistics with limited machinery (optimized but capped).

Modern LLM deployment needs both: capable models AND efficient serving.

== In Practice: Serving LLMs with OpenShift AI

The serving systems you've learned about — vLLM, TGI, OpenVINO — are all available in *Red Hat OpenShift AI* as managed runtimes. The platform handles the infrastructure complexity so you can focus on deploying models.

=== The Single-Model Serving Platform

OpenShift AI provides a *single-model serving platform* based on KServe. Each LLM gets its own dedicated model server, enabling:

* Independent scaling per model
* GPU resource isolation
* Model-specific configurations
* REST and gRPC API endpoints

=== Key Components

The serving stack has three layers:

[cols="1,1", options="header"]
|===
|Component
|Role

|*ServingRuntime*
|Defines the container image and configuration (e.g., vLLM)

|*InferenceService*
|Deploys a specific model using a runtime

|*KServe*
|Orchestrates lifecycle, routing, and scaling
|===

=== Understanding the Three Layers

These three components solve different problems and operate at different levels. Understanding their boundaries prevents confusion.

==== KServe: The Control Plane

KServe is the orchestration layer. Think of it as the manager that ensures models are running, healthy, and accessible.

*What KServe owns:*

* Deploying model server pods
* Autoscaling (adding/removing replicas)
* Health checks and restarts
* Traffic routing between versions
* Exposing external endpoints

*What KServe does NOT do:*

* Load model weights
* Run GPU computations
* Manage the KV cache
* Generate tokens
* Touch the inference math

KServe manages model servers the way Kubernetes manages pods. It doesn't replace the model server — it operates it.

==== Model Server: The Execution Engine

The model server (vLLM, Triton, TGI) is where inference actually happens. This is the component that uses your GPU.

*What the model server owns:*

* Loading model weights into GPU memory
* Running the tokenizer
* Executing transformer forward passes
* Managing the KV cache
* Applying PagedAttention optimizations
* Generating tokens one by one

*The key insight:* When you deploy a model through KServe, KServe launches a model server pod. The model server does all the work described in Chapters 9-11. KServe just makes sure that pod is running and accessible.

==== InferenceService: The Abstraction

An InferenceService is the Kubernetes resource that ties everything together. It specifies:

* Which model to serve
* Which runtime to use (vLLM, OpenVINO, etc.)
* Resource requirements (GPUs, memory)
* Scaling parameters

When you create an InferenceService, KServe reads it and creates the necessary pods, services, and routes.

=== InferenceService vs OpenShift Route

A common point of confusion: how is an InferenceService different from an OpenShift Route?

[cols="1,1,1", options="header"]
|===
|Aspect
|OpenShift Route
|InferenceService

|*Layer*
|Networking
|Application/ML serving

|*Purpose*
|Expose HTTP(S) traffic
|Serve models with lifecycle management

|*ML awareness*
|None
|Full (knows about models, runtimes, GPUs)

|*Autoscaling*
|No
|Yes

|*GPU awareness*
|No
|Yes

|*Health checks*
|Basic HTTP
|Model-specific readiness
|===

An InferenceService _uses_ Routes (indirectly) to expose its endpoint. But they solve different problems:

* A Route is networking plumbing — it moves HTTP traffic.
* An InferenceService defines and operates the inference workload that traffic reaches.

You could deploy a model using just a Route and a plain Deployment, but you'd lose autoscaling, model lifecycle management, and the serving abstractions that make production deployment practical.

=== Where Service Mesh Fits

If you're using Istio or another service mesh, you might wonder: does the mesh replace KServe?

No — they complement each other.

[cols="1,1,1", options="header"]
|===
|Technology
|Layer
|Provides

|*Istio/Envoy*
|Network/service mesh
|mTLS, authentication, traffic policies, observability

|*KServe*
|Model serving control plane
|Model lifecycle, inference abstraction, ML-aware scaling
|===

Istio can sit in front of KServe, providing security and observability at the network layer. KServe handles the model-serving concerns that a generic service mesh doesn't understand.

=== vLLM in OpenShift AI

Remember PagedAttention from earlier? It's available via the *vLLM ServingRuntime for KServe*:

[source,yaml]
----
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: vllm-cuda-runtime
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    openshift.io/display-name: vLLM NVIDIA GPU ServingRuntime for KServe
spec:
  containers:
  - name: kserve-container
    image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:latest
    command: ["python", "-m", "vllm.entrypoints.openai.api_server"]
    args:
    - --port=8080
    - --model=/mnt/models
    - --served-model-name={{.Name}}
    ports:
    - containerPort: 8080
      protocol: TCP
  supportedModelFormats:
  - name: vLLM
    autoSelect: true
----

This ServingRuntime tells OpenShift AI how to run vLLM containers. The image uses the official Red Hat AI Inference Server.

=== Deploying a Model

To deploy an LLM, create an *InferenceService* that references your model. OpenShift AI 3.x supports OCI-based model URIs via the Model Catalog:

[source,yaml]
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-32-3b-instruct
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
    opendatahub.io/hardware-profile-name: gpu-profile
    opendatahub.io/model-type: generative
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 1
    model:
      modelFormat:
        name: vLLM
      runtime: vllm-cuda-runtime
      storageUri: oci://quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-3b-instruct
      args:
      - --dtype=half
      - --max-model-len=20000
      - --gpu-memory-utilization=0.95
      resources:
        requests:
          nvidia.com/gpu: "1"
          memory: 6Gi
        limits:
          nvidia.com/gpu: "1"
          memory: 16Gi
----

This creates:

* A pod running vLLM with your model loaded via OCI registry
* A Kubernetes Service for internal access
* An external Route for API access
* RawDeployment mode (the recommended mode in RHOAI 3.x)

=== Available Model-Serving Runtimes

OpenShift AI 3.x includes these pre-installed runtimes:

[cols="1,1,1", options="header"]
|===
|Runtime
|Best For
|Protocol

|*vLLM ServingRuntime*
|LLMs with PagedAttention (recommended for LLMs)
|REST (OpenAI-compatible)

|*OpenVINO Model Server*
|Intel-optimized inference
|REST

|*Distributed Inference with llm-d*
|High-throughput distributed LLM serving
|REST
|===

You can also add custom runtimes for specific model frameworks.

=== Model Catalog vs. Model Registry

OpenShift AI 3.x provides two ways to discover and deploy models:

[cols="1,1,1", options="header"]
|===
|Source
|What It Contains
|Use Case

|*Model Catalog*
|Pre-built Red Hat models (Llama, Granite, Mistral, embedding models)
|Quick deployment of curated, tested models

|*Model Registry*
|Your organization's trained/fine-tuned models
|Deploy models you've built or customized
|===

*Model Catalog* → Deploy with one click from the dashboard. Models are pulled from OCI registries.

*Model Registry* → Register models after training (Chapter 10), then deploy any version. Supports rollbacks, A/B testing, and audit trails.

Both integrate with the deployment wizard in Gen AI Studio, so the serving experience is consistent regardless of source.

=== Calling Your Deployed Model

Once deployed, the InferenceService exposes OpenAI-compatible endpoints:

[source,bash]
----
# Chat completions endpoint
curl -X POST https://llama-7b-myproject.apps.cluster.example.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-7b",
    "messages": [
      {"role": "user", "content": "Explain transformers in one paragraph."}
    ],
    "max_tokens": 256,
    "temperature": 0.7
  }'
----

The response streams back just like calling OpenAI's API — but running on your own infrastructure.

=== GPU Acceleration Options

OpenShift AI 3.x supports multiple accelerators via *Hardware Profiles*:

[cols="1,1,1", options="header"]
|===
|Accelerator
|Support Level
|Examples

|*NVIDIA GPU*
|Fully Supported
|T4, A10, A100, L4, L40S, H100, H200, B200

|*AMD GPU*
|Fully Supported
|MI210, MI300X

|*Intel Gaudi*
|Fully Supported
|Gaudi 2, Gaudi 3

|*Google TPU*
|Technology Preview
|v4, v5e, v6e

|*IBM Spyre*
|Supported
|Power, Z
|===

The platform handles driver installation and device allocation automatically through the respective GPU Operators (NVIDIA GPU Operator, AMD GPU Operator, etc.).

=== Autoscaling

With KServe RawDeployment mode (the default in OpenShift AI 3.x), models scale using Kubernetes HPA (Horizontal Pod Autoscaler):

* *Scale up*: Traffic spike? Add replicas automatically based on metrics.
* *Scale down*: Low traffic? Reduce replicas to save resources.
* *Scale limits*: Cap maximum replicas to control spend.

[source,yaml]
----
spec:
  predictor:
    minReplicas: 1    # Minimum replicas
    maxReplicas: 4    # Maximum replicas
    scaleTarget: 1    # Concurrent requests per replica for scaling
----

Note: Scale-to-zero is available via the Serverless deployment mode, but RawDeployment is recommended for production LLM workloads as it provides more predictable performance.

=== Architecture Overview

[source]
----
┌──────────────────────────────────────────────────────────────┐
│                    OpenShift AI Platform                      │
│  ┌─────────────────────────────────────────────────────────┐ │
│  │                  Data Science Project                    │ │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────────┐  │ │
│  │  │  Workbench  │  │    Data     │  │     Model       │  │ │
│  │  │ (Training)  │──│  Connection │──│    Server       │  │ │
│  │  │             │  │   (S3)      │  │   (vLLM)        │  │ │
│  │  └─────────────┘  └─────────────┘  └────────┬────────┘  │ │
│  └─────────────────────────────────────────────│───────────┘ │
│                                                │              │
│  ┌─────────────────────────────────────────────│───────────┐ │
│  │                    KServe                   │            │ │
│  │  ┌───────────────┐  ┌───────────────┐  ┌───▼─────────┐  │ │
│  │  │ServingRuntime │  │InferenceService│  │   Route    │  │ │
│  │  │    (vLLM)     │──│  (llama-7b)   │──│  (HTTPS)   │  │ │
│  │  └───────────────┘  └───────────────┘  └───────────┘   │ │
│  └─────────────────────────────────────────────────────────┘ │
└──────────────────────────────────────────────────────────────┘
                              ▲
                              │ REST/gRPC
                    ┌─────────┴─────────┐
                    │  User/Application │
                    └───────────────────┘
----

=== Distributed Inference with llm-d

For high-throughput scenarios or large models that span multiple GPUs, OpenShift AI 3.x offers *llm-d* (distributed inference). This uses a separate CRD:

[source,yaml]
----
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: qwen3-sample
  labels:
    kueue.x-k8s.io/queue-name: default
    opendatahub.io/genai-asset: "true"
spec:
  replicas: 1
  model:
    uri: oci://registry.redhat.io/rhelai1/modelcar-qwen3-8b-fp8-dynamic:latest
    name: RedHatAI/Qwen3-8B-FP8-dynamic
  router:
    route: {}
    gateway: {}
  scheduler: {}
  template:
    containers:
    - name: main
      resources:
        limits:
          nvidia.com/gpu: "1"
          memory: 32Gi
----

llm-d provides:

* *Intelligent routing* across model replicas
* *Multi-node/multi-GPU* support for Mixture of Experts (MoE) models
* *Integration with Kueue* for workload scheduling
* *Gateway API* for ingress with authentication

=== Why This Matters

Without a platform like OpenShift AI, deploying vLLM means:

* Managing Kubernetes YAML manually
* Configuring GPU node taints/tolerations
* Setting up load balancers and TLS
* Building monitoring and logging pipelines
* Handling authentication and authorization

OpenShift AI abstracts this complexity. You get vLLM's PagedAttention benefits with enterprise-grade operations.

=== GitOps for AI Infrastructure

Enterprise deployments take this further with *GitOps* — managing the entire AI platform through version-controlled YAML files. This approach provides:

* *Infrastructure Reproducibility*: Dev and prod clusters are identical
* *Audit Trail*: Complete history of all configuration changes
* *Automated Rollbacks*: Revert to previous configurations instantly
* *Declarative Model Serving*: Models deployed as code

A GitOps-managed AI platform might organize model deployments like this:

[source]
----
components/configs/model-serving/
├── granite-3.3-8b-instruct.yaml    # Granite LLM deployment
├── llama-3.2-3b.yaml               # Llama deployment
├── mistral-small-24b.yaml          # Mistral deployment
├── nomic-embed-text-v1-5.yaml      # Embedding model
└── serving-runtimes/
    ├── vllm-runtime.yaml           # vLLM ServingRuntime
    └── openvino-runtime.yaml       # OpenVINO ServingRuntime
----

Each model is a declarative InferenceService:

[source,yaml]
----
# granite-3.3-8b-instruct.yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: granite-3-3-8b-instruct
  annotations:
    serving.kserve.io/autoscalerClass: hpa
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 4
    model:
      modelFormat:
        name: vLLM
      runtime: vllm-runtime
      storageUri: s3://models/granite-3.3-8b-instruct/
      resources:
        limits:
          nvidia.com/gpu: "1"
          memory: 48Gi
----

Git becomes the single source of truth. To deploy a new model:

. Add a YAML file to the repository
. Create a pull request for review
. Merge to main — ArgoCD automatically deploys
. Model is live with full audit trail

This pattern scales to dozens of models across multiple environments, with GPU autoscaling policies and API gateway configurations all managed declaratively.

== Chapter Takeaway

[NOTE]
====
*Serving systems optimize LLM deployment* through KV cache management, batching, and memory efficiency. vLLM's PagedAttention treats the KV cache like virtual memory, eliminating fragmentation. These systems don't change the model — they make running it practical at scale. *OpenShift AI provides vLLM and other runtimes as managed ServingRuntimes*, with KServe orchestrating deployment, scaling, and API exposure.
====

== Part VI Summary

You've seen the engineering that makes LLMs practical:

. *KV cache* eliminates redundant computation by storing K/V vectors
. *Serving systems* optimize deployment through batching and memory management

These are systems concerns, separate from the ML itself. But essential for real-world use.

Now we bring it all together.

xref:13-vocabulary.adoc[Next: The Complete Vocabulary →]
